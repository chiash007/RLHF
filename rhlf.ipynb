{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "LV4-CQQSsU6S",
   "metadata": {
    "id": "LV4-CQQSsU6S"
   },
   "source": [
    "MODEL LINK : https://huggingface.co/bigcode/tiny_starcoder_py\n",
    "\n",
    "DATASET FOR STEP 2: https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\n",
    "\n",
    "DATASET FOR STEP 1 and 3: https://huggingface.co/datasets/CarperAI/openai_summarize_tldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4dc5854e-0436-4d76-b3bb-728c4b81cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(65298) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m458.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m665.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.8/381.8 kB\u001b[0m \u001b[31m835.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m507.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.2 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fa80b0e",
   "metadata": {
    "id": "2fa80b0e"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:38\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[1;32m     44\u001b[0m     hp_params,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m      9\u001b[0m     AutoTokenizer,\n\u001b[1;32m     10\u001b[0m     Trainer,\n\u001b[1;32m     11\u001b[0m     TrainingArguments,\n\u001b[1;32m     12\u001b[0m     default_data_collator,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_seed\u001b[39m(seed_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[1;32m     17\u001b[0m     random\u001b[38;5;241m.\u001b[39mseed(seed_val)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "def set_seed(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "eval_batch_size = 1\n",
    "eval_steps = 500\n",
    "max_input_length = 550\n",
    "save_steps = 1000\n",
    "num_train_epochs = 20\n",
    "random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c6ca",
   "metadata": {
    "id": "4018c6ca"
   },
   "source": [
    "## Creating the policy model for human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90da8c2",
   "metadata": {
    "id": "f90da8c2"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/test_policy.parquet\") ## downloded above linked dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00044d2",
   "metadata": {
    "id": "b00044d2",
    "outputId": "da91fb70-a9b4-47cf-d6a9-92f85b2a3ffe"
   },
   "outputs": [],
   "source": [
    "df.iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670ff5e",
   "metadata": {
    "id": "0670ff5e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TLDRDataset(Dataset):\n",
    "    def __init__(self, train_path, tokenizer, split, max_length=256):\n",
    "        self.post_list = []\n",
    "        dataset = pd.read_parquet(train_path)\n",
    "        self.labels = []\n",
    "#         dataset = dataset[:100]\n",
    "        for sample in dataset.iterrows():\n",
    "            self.post_list.append(sample[1][\"prompt\"])\n",
    "            self.labels.append(sample[1][\"label\"])\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.post_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.post_list[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encodings_dict = self.tokenizer(txt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        encodings_dict_label = self.tokenizer(label,truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
    "        attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
    "        labels_ids = torch.tensor(encodings_dict_label[\"input_ids\"])\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_masks,\n",
    "            \"labels\": labels_ids,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced6616",
   "metadata": {
    "id": "bced6616"
   },
   "outputs": [],
   "source": [
    "# for i in TLDRDataset():\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a73ab",
   "metadata": {
    "id": "7f0a73ab"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/tiny_starcoder_py\", use_cache=False).to(\"cuda:1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6aad5",
   "metadata": {
    "id": "dbc6aad5"
   },
   "outputs": [],
   "source": [
    "# Set up the datasets\n",
    "data_path = \"test.parquet\"\n",
    "train_dataset = TLDRDataset(\n",
    "    data_path,\n",
    "    tokenizer,\n",
    "    \"train\",\n",
    "    max_length=256,\n",
    ")\n",
    "# dev_dataset = TLDRDataset(\n",
    "#     data_path,\n",
    "#     tokenizer,\n",
    "#     \"valid\",\n",
    "#     max_length=max_input_length,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a19aa",
   "metadata": {
    "id": "f16a19aa",
    "outputId": "dbf1ac52-40a3-4d6d-b047-5fc7f952aa22"
   },
   "outputs": [],
   "source": [
    "for i in train_dataset:\n",
    "    print(i[\"input_ids\"], i[\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500301ac",
   "metadata": {
    "id": "500301ac"
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3801788",
   "metadata": {
    "id": "d3801788"
   },
   "outputs": [],
   "source": [
    "# Prepare the trainer and start training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "#     per_device_eval_batch_size=eval_batch_size,\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefacaf",
   "metadata": {
    "id": "baefacaf",
    "outputId": "c42dfdf0-63a9-44d1-ac64-53ceff277e65"
   },
   "outputs": [],
   "source": [
    "training_args.device.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba4710",
   "metadata": {
    "id": "90ba4710",
    "outputId": "473b9305-5aea-447d-e46f-874f06f82ee9"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=default_data_collator,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "trainer.train()\n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc50c2",
   "metadata": {
    "id": "3dfc50c2"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"summarization_policy_new/\")   ##path to save policy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373c19c",
   "metadata": {
    "id": "b373c19c",
    "outputId": "dfc51c93-f199-4db2-89d1-cc763f31dc2b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"summarization_policy_new/\")\n",
    "model_path = \"bigcode/tiny_starcoder_py\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, max_length=256, padding=\"max_length\")\n",
    "text = df.iloc[2][\"prompt\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7fe18",
   "metadata": {
    "id": "6dc7fe18",
    "outputId": "e761b8b9-c69a-4459-f6c9-02c3633b84d7"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(model.generate(**tokenized_text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421708e",
   "metadata": {
    "id": "8421708e"
   },
   "source": [
    "## Traning the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b607ce4",
   "metadata": {
    "id": "5b607ce4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer, SFTTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849ca0f",
   "metadata": {
    "id": "e849ca0f"
   },
   "outputs": [],
   "source": [
    "##model path\n",
    "MODEL_PATH = \"bigcode/tiny_starcoder_py\"\n",
    "DATA_PATH = \"data/test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddb7e5",
   "metadata": {
    "id": "14ddb7e5",
    "outputId": "8001d1b3-25fc-4542-a827-bc00e6f1bc93"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "df = df[:10]\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209d915",
   "metadata": {
    "id": "2209d915"
   },
   "outputs": [],
   "source": [
    "##defininig the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368defaa",
   "metadata": {
    "id": "368defaa"
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "def formatting_func(examples):\n",
    "    kwargs = {\"padding\": \"max_length\",\n",
    "              \"truncation\": True,\n",
    "              \"max_length\": 256,\n",
    "              \"return_tensors\": \"pt\"\n",
    "              }\n",
    "\n",
    "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
    "    prompt_plus_chosen_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n",
    "    prompt_plus_rejected_response = examples[\"prompt\"] + \"\\n\" + examples[\"rejected\"]\n",
    "\n",
    "    # Then tokenize these modified fields.\n",
    "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
    "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
    "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4afbee",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "0f4afbee",
    "outputId": "2cc1a31c-16b7-47cd-e5ae-e6e160386fa4"
   },
   "outputs": [],
   "source": [
    "formatted_dataset = raw_dataset.map(formatting_func)\n",
    "formatted_dataset = formatted_dataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43490bac",
   "metadata": {
    "id": "43490bac",
    "outputId": "f3d87057-0f54-478c-9e91-23ad6b61741e"
   },
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3aef6",
   "metadata": {
    "id": "dce3aef6"
   },
   "outputs": [],
   "source": [
    "### Loading the TRL reward trainer and training the trainer\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"rm_checkpoint/\",\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=10,\n",
    "        gradient_accumulation_steps=1,\n",
    "        save_strategy=\"steps\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        eval_accumulation_steps=1,\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        warmup_steps=100,\n",
    "        logging_dir=\"./logs\",\n",
    "        learning_rate=1e-5,\n",
    "        save_total_limit=1,\n",
    "        no_cuda=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ad45a",
   "metadata": {
    "id": "d06ad45a",
    "outputId": "ec58bb3d-6bae-41d7-8485-a6cd24ac6eaf"
   },
   "outputs": [],
   "source": [
    "trainer = RewardTrainer(model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        train_dataset=formatted_dataset['train'],\n",
    "                        eval_dataset=formatted_dataset['test'],\n",
    "                        args= training_args\n",
    "                        )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8620e81",
   "metadata": {
    "id": "c8620e81"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"rm_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56a13c",
   "metadata": {
    "id": "1d56a13c"
   },
   "outputs": [],
   "source": [
    "## inference the model\n",
    "rm_model = AutoModelForCausalLM.from_pretrained(\"rm_model/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694fbc6",
   "metadata": {
    "id": "e694fbc6"
   },
   "outputs": [],
   "source": [
    "def get_score(model, tokenizer, prompt, response):\n",
    "\n",
    "    instructions = tokenizer.encode_plus(prompt,\n",
    "                                       response,\n",
    "                                       padding=\"max_length\",\n",
    "                                       max_length=256,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                        truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**instructions)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed481305",
   "metadata": {
    "id": "ed481305"
   },
   "outputs": [],
   "source": [
    "# usage with prompt\n",
    "prompt = df.iloc[0][\"prompt\"]\n",
    "example_prefered_response = df.iloc[0][\"chosen\"]\n",
    "example_unprefered_response = df.iloc[0][\"rejected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d513768",
   "metadata": {
    "id": "1d513768"
   },
   "outputs": [],
   "source": [
    "loss1 = get_score(model, tokenizer, prompt, example_prefered_response)\n",
    "loss2= get_score(model, tokenizer, prompt, example_unprefered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95e92f",
   "metadata": {
    "id": "bc95e92f"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = -nn.functional.logsigmoid(loss1 - loss2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e14b4",
   "metadata": {
    "id": "1c5e14b4",
    "outputId": "9e19b9f4-8eae-4174-cfc3-1be26519d101"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.max(loss1, axis=-1).indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92769534",
   "metadata": {
    "id": "92769534"
   },
   "source": [
    "# Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cb217",
   "metadata": {
    "id": "bb9cb217"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer, SFTTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0acdb",
   "metadata": {
    "id": "50d0acdb"
   },
   "outputs": [],
   "source": [
    "##model path\n",
    "MODEL_PATH = \"rm_model/\"\n",
    "DATA_PATH = \"data/test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cfe68",
   "metadata": {
    "id": "221cfe68",
    "outputId": "26ab818a-ffcf-4594-82d5-0073c32c66aa"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "df = df[:1000]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f02097",
   "metadata": {
    "id": "05f02097"
   },
   "outputs": [],
   "source": [
    "sentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=MODEL_PATH, steps=51200, learning_rate=1.41e-5, remove_unused_columns=True\n",
    ")\n",
    "\n",
    "txt_in_len = 5\n",
    "txt_out_len = 20\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5769b3",
   "metadata": {
    "id": "6e5769b3"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0ef6a",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "40d0ef6a",
    "outputId": "a3f689c0-c3e9-47c3-c74e-0f9a5fae662f"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"prompt\": \"review\"})\n",
    "dataset = dataset.filter(lambda x: len(x[\"review\"]) > 500, batched=False)\n",
    "dataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7e8b8",
   "metadata": {
    "id": "96d7e8b8"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adda4dc",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "3adda4dc",
    "outputId": "c10d2a3c-9143-4e2c-8953-7cc286e5b9c8"
   },
   "outputs": [],
   "source": [
    "txt_in_len = 5\n",
    "txt_out_len = 32\n",
    "seed = 1\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"input_ids\": tokenizer.encode(\" \" + x[\"chosen\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=32)[0]},\n",
    "    batched=False,\n",
    ")\n",
    "dataset = dataset.map(lambda x: {\"query\": tokenizer.decode(x[\"input_ids\"])}, batched=False)\n",
    "dataset = dataset[:20480]\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "dataset.set_format(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189a22b",
   "metadata": {
    "id": "e189a22b"
   },
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc612f5f",
   "metadata": {
    "id": "cc612f5f"
   },
   "outputs": [],
   "source": [
    "rf_model_path = \"rm_model/\"\n",
    "starcoder_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"summarization_policy_new/\")  ##policy model from step 1\n",
    "starcoder_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(rf_model_path) ## reward model from step 2\n",
    "starcoder_tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\") ## tokenizer of step 1 model., here since we are using same model for step 1 and 2 it doesnot matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836e5cc",
   "metadata": {
    "id": "2836e5cc",
    "outputId": "f8ea4bec-594c-4c55-8952-56b6731a90fc"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800d3e9",
   "metadata": {
    "id": "4800d3e9"
   },
   "outputs": [],
   "source": [
    "# starcoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a2e38",
   "metadata": {
    "id": "2e1a2e38"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.SGD(starcoder_model.parameters(), lr=config.learning_rate)\n",
    "ppo_trainer = PPOTrainer(config, starcoder_model, starcoder_model, starcoder_tokenizer, dataset=dataset, data_collator=collator, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e72ad",
   "metadata": {
    "id": "e63e72ad"
   },
   "outputs": [],
   "source": [
    "# for i in ppo_trainer.dataloader:\n",
    "#   print(i)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329e292",
   "metadata": {
    "id": "e329e292"
   },
   "outputs": [],
   "source": [
    "ctrl_str = [\"[negative]\", \"[positive]\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # this should be handled by accelerate\n",
    "ctrl_tokens = dict((s, starcoder_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ac197",
   "metadata": {
    "id": "597ac197"
   },
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [neutral]: reward = -2*abs(logit)+4\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i] == \"[negative]\":\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i] == \"[positive]\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"task has to be in [0, 1, 2]!\")\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a916542",
   "metadata": {
    "id": "8a916542",
    "outputId": "b3fbbee8-1fa3-498f-fe7e-15c2d39702b2"
   },
   "outputs": [],
   "source": [
    "pos_logit_to_reward(torch.Tensor([4, 4]), ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974cb44",
   "metadata": {
    "id": "2974cb44"
   },
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": starcoder_tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"eos_token_id\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf0762",
   "metadata": {
    "id": "27cf0762"
   },
   "outputs": [],
   "source": [
    "def get_score(model, tokenizer, responses):\n",
    "    positive_logist = []\n",
    "    for i in responses:\n",
    "        instructions = tokenizer.encode_plus(\n",
    "                                           i,\n",
    "                                           padding=\"max_length\",\n",
    "                                           max_length=32,\n",
    "                                           return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**instructions)\n",
    "\n",
    "        logits = outputs[0].mean()\n",
    "        positive_logist.append(logits)\n",
    "\n",
    "    return positive_logist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d47d37",
   "metadata": {
    "id": "41d47d37"
   },
   "outputs": [],
   "source": [
    "# responses =[\"ashish is a goo\", \"heelow how are you\", \"__IT_\\nr/\\n: r RelationshipRelationship]]0]\\nlsriend\\n2//M]\\n [ [ a\\n the was to the [. a friends to\\n\\n:\\n [lfriend [ me have a aried in his19 minutes.\\n\\nWhat Modified:** girlfriend was through the Facebook.. I my my friends.**** my  of lf**\\n\\n** was d1ing for my few personirl** I had for findoolpping my my the future** but I was that in\\n\\n** have ali  of to she  tolirt my me girl. and she found my about my.. me few of gir.1viously). was\\'t find her was).\\n\\n** was it about my twoirl and the had  Facebook. the  and she gand historyirl) was in April,\\n to, find, were flirted. I a messages.. f.ing on her.\\n girlM\\n; I1 girirllfriend and the19 months. to my Facebook.. my permission. she her messages. my.lirty with my fewirl.\\n found her with me. I through more with\\n\"]\n",
    "# get_score(starcoder_model, tokenizer, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e99bd",
   "metadata": {
    "id": "8b9e99bd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d6cc8",
   "metadata": {
    "id": "253d6cc8",
    "outputId": "84898817-5381-4656-d0de-75c7dc80803d"
   },
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        (logs, game_data,) = (\n",
    "            dict(),\n",
    "            dict(),\n",
    "        )\n",
    "\n",
    "        print(ctrl_str)\n",
    "        #### prepend a random control token\n",
    "        task_list = choices(ctrl_str, k=config.batch_size)\n",
    "        game_data[\"query\"] = [t + q for t, q in zip(task_list, batch[\"query\"])]\n",
    "        query_tensors = [torch.cat((ctrl_tokens[t], input_ids)) for t, input_ids in zip(task_list, batch[\"input_ids\"])]\n",
    "\n",
    "        #### get response from gpt2\n",
    "        response_tensors = []\n",
    "        for query in query_tensors:\n",
    "            response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "            response_tensors.append(response.squeeze()[-txt_out_len:])\n",
    "#         print(response_tensors)\n",
    "        game_data[\"response\"] = [starcoder_tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "        #### sentiment analysis\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], game_data[\"response\"])]\n",
    "        logits = get_score(starcoder_model,starcoder_tokenizer, texts)\n",
    "        rewards = pos_logit_to_reward(logits, task_list)\n",
    "\n",
    "        #### Run PPO training\n",
    "        t = time.time()\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "\n",
    "        for cs in ctrl_str:\n",
    "            key = \"env/reward_\" + cs.strip(\"[]\")\n",
    "            stats[key] = np.mean([r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs])\n",
    "        ppo_trainer.log_stats(stats, game_data, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bad48f",
   "metadata": {
    "id": "d0bad48f",
    "outputId": "dba21dc2-3836-4df1-a87c-f201eaf126fe"
   },
   "outputs": [],
   "source": [
    "###saving the model\n",
    "starcoder_model.save_pretrained(\"rhlfmodel/\")\n",
    "starcoder_tokenizer.save_pretrained(\"rhlfmodel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf2489",
   "metadata": {
    "id": "23bf2489",
    "outputId": "314e7241-83ab-4e3e-ccb2-79fe49813dd4"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "model_path = \"rhlfmodel/\"\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\",model=model_path, tokenizer=model_path, max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e2c6c",
   "metadata": {
    "id": "026e2c6c"
   },
   "outputs": [],
   "source": [
    "# text = dataset[\"rejected\"][0]\n",
    "# pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6a2e4",
   "metadata": {
    "id": "85b6a2e4"
   },
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d8135",
   "metadata": {
    "id": "8e8d8135",
    "outputId": "c7f23370-09e6-4305-eee5-32428a000dd7"
   },
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f478497",
   "metadata": {
    "id": "2f478497"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
